{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "demonstrated-theta",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_toolkit import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "behind-bonus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allowed hyperparameter values are as follows\n",
    "allowed_activation_function = ['relu', 'sigmoid', 'linear', 'tanh', 'softmax']\n",
    "allowed_weight_init = ['zero', 'random', 'he', 'xavier']\n",
    "allowed_optimizer = ['gradient_descent', 'gradient_descent_with_momentum', 'NAG', 'AdaGrad', 'RMSProp', 'Adam']\n",
    "allowed_regularization = ['l1', 'l2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "norman-modern",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initilaize kwargs dictionary to send additional parameters\n",
    "kwargs = {\n",
    "    'beta' : 0.9,             # for momentum\n",
    "    'gamma' : 0.9,            # for RMSProp and Adam\n",
    "    'epsilon' : 10**-8,       # for optimizers (adding into denominator to prevent divide by 0 error)\n",
    "    'lamda' : 0               # regularization term\n",
    "}  \n",
    "\n",
    "\n",
    "# Initializing the model and assigning hyperparameters\n",
    "mnn = MLPClassifier(layers = [784, 256, 128, 64, 10],       # 784 is input features and 10 is no. of classes\n",
    "                    learning_rate = 0.001, \n",
    "                    activation_function = \"tanh\", \n",
    "                    optimizer = 'RMSProp',\n",
    "                    weight_init = \"xavier\", \n",
    "                    regularization = 'l2',\n",
    "                    batch_size = 64, \n",
    "                    num_epochs = 5,\n",
    "                    dropouts = 0.2, \n",
    "                    **kwargs\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "isolated-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading the MNIST dataset from sklearn\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784')\n",
    "\n",
    "x, y = mnist.data, mnist.target\n",
    "x, y = x.astype(np.int32).to_numpy(), y.astype(np.int32).to_numpy()   # converting x, y to int32 numpy array\n",
    "\n",
    "# splitting the data in train and test (Stratified sampling)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, stratify = y, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-channel",
   "metadata": {},
   "source": [
    "**Example 1**\n",
    "*(Printing only training accuracy and entropy after each epoch)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "antique-launch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training cost at 0 th iteration: 0.03586947815427803\n",
      "Training Accuracy at 0 th iteration: 0.9102678571428572\n",
      "-------------------------\n",
      "Training cost at 1 th iteration: 0.026838658120421993\n",
      "Training Accuracy at 1 th iteration: 0.9318571428571428\n",
      "-------------------------\n",
      "Training cost at 2 th iteration: 0.022816801671905595\n",
      "Training Accuracy at 2 th iteration: 0.9402678571428571\n",
      "-------------------------\n",
      "Training cost at 3 th iteration: 0.021164635642091666\n",
      "Training Accuracy at 3 th iteration: 0.944375\n",
      "-------------------------\n",
      "Training cost at 4 th iteration: 0.018190515847569238\n",
      "Training Accuracy at 4 th iteration: 0.9509642857142857\n",
      "-------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<dl_toolkit.MLPClassifier at 0x10f422e20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# running the model\n",
    "mnn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-saudi",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fitted-shepherd",
   "metadata": {},
   "source": [
    "**Example 2**\n",
    "*(Printing both training and testing accuracy and entropy after each epoch)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "continuing-thesaurus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training cost at 0 th iteration: 0.03386245487856913\n",
      "Training Accuracy at 0 th iteration: 0.9218928571428572\n",
      "Testing cost at 0 th iteration: 0.03616672887753292\n",
      "Testing Accuracy at 0 th iteration: 0.918\n",
      "-------------------------\n",
      "Training cost at 1 th iteration: 0.02609995234942342\n",
      "Training Accuracy at 1 th iteration: 0.9343035714285715\n",
      "Testing cost at 1 th iteration: 0.029218196374709043\n",
      "Testing Accuracy at 1 th iteration: 0.9305714285714286\n",
      "-------------------------\n",
      "Training cost at 2 th iteration: 0.02304371794268036\n",
      "Training Accuracy at 2 th iteration: 0.9414642857142858\n",
      "Testing cost at 2 th iteration: 0.025158432857449002\n",
      "Testing Accuracy at 2 th iteration: 0.9372142857142857\n",
      "-------------------------\n",
      "Training cost at 3 th iteration: 0.020444594945076448\n",
      "Training Accuracy at 3 th iteration: 0.9471607142857142\n",
      "Testing cost at 3 th iteration: 0.02362417350038051\n",
      "Testing Accuracy at 3 th iteration: 0.9408571428571428\n",
      "-------------------------\n",
      "Training cost at 4 th iteration: 0.017485816374194434\n",
      "Training Accuracy at 4 th iteration: 0.9532857142857143\n",
      "Testing cost at 4 th iteration: 0.020358327958163506\n",
      "Testing Accuracy at 4 th iteration: 0.9483571428571429\n",
      "-------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<dl_toolkit.MLPClassifier at 0x10f422e20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# running the model\n",
    "mnn.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-tanzania",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
